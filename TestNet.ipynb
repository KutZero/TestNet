{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d0315c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bac92a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLPRegressor(hidden_layer_sizes=(900,7), activation='relu',\n",
    "#             solver='adam',alpha=0.001, \n",
    "#             batch_size=13, learning_rate='constant',learning_rate_init=0.001, \n",
    "#             max_iter=1000, shuffle=True,random_state=5, tol=0.001, \n",
    "#             early_stopping=True,\n",
    "#             verbose=True,\n",
    "#             validation_fraction=0.2\n",
    "#             )\n",
    "cur_hidden_layer_sizes = (900,7)\n",
    "cur_act_fun = 'reLu'\n",
    "cur_solver = 'adam'\n",
    "cur_alpha = 0.001\n",
    "cur_batch_size = 13\n",
    "cur_learning_rate = 'constant'\n",
    "cur_learning_rate_init = 0.001\n",
    "cur_max_iter = 1000\n",
    "cur_shuffle = True\n",
    "cur_random_state = 5\n",
    "cur_tol = 0.001\n",
    "cur_early_stopping = True\n",
    "cur_verbose = True\n",
    "cur_validation_fraction = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887b1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    file = pd.read_csv('ngp.csv')\n",
    "    return pd.DataFrame(list(reversed(file['price'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd481b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols_and_names(df):\n",
    "    input_t = 4\n",
    "    output_t = 1\n",
    "    \n",
    "    cols = list()\n",
    "    names = list()\n",
    "    print(\"///////// get_cols_and_names /////////\")\n",
    "    for i in range(input_t,0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        #print(f\"df.shift({i}): \",df.shift(i))\n",
    "        print(f\"df.shift({i}).shape: \", df.shift(i).shape)\n",
    "        names.append(i)\n",
    "    #print(type(cols)\n",
    "    #print(\"Cols: \", np.array(cols).shape())\n",
    "    print(\"///////// Next loop /////////\")\n",
    "    for i in range(0,output_t):\n",
    "        cols.append(df.shift(-i))\n",
    "        print(f\"df.shift({-i}).shape: \",df.shift(-i).shape)\n",
    "        names.append(i+input_t+1)\n",
    "        \n",
    "    #a = np.array(cols)\n",
    "    print(\"\\ncols.shape: \", np.array(cols).shape)\n",
    "    print(\"names: \", names)\n",
    "    \n",
    "    print(\"len(cols)\", len(cols))\n",
    "    print(\"len(cols[0])\", len(cols[0]))\n",
    "    print(\"len(cols[0][0])\", len(cols[0][0]))\n",
    "    \n",
    "    print(\"Cols: \", cols)\n",
    "    return cols,names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1397f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(X,Y,factor):\n",
    "    X_train=X[:factor]\n",
    "    Y_train=Y[:factor]\n",
    "    X_test=X[factor:]\n",
    "    Y_test=Y[factor:]\n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "402ec412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_4_1_time():\n",
    "    #file = pd.read_csv('ngp.csv')\n",
    "    #df = pd.DataFrame(list(reversed(file['price'])))\n",
    "    print(\"///////// prep_data_4_1_time /////////\")\n",
    "    \n",
    "    df = get_df()\n",
    "    \n",
    "    cols, names = get_cols_and_names(df)\n",
    "    \n",
    "    #print(\"Cols: \", cols)\n",
    "\n",
    "    #print(\"Names: \", names)\n",
    "    \n",
    "    #cols = list()\n",
    "    #names = list()\n",
    "   # \n",
    "   # for i in range(input_t,0, -1):\n",
    "   #     cols.append(df.shift(i))\n",
    "   #     names.append(i)\n",
    "   # for i in range(0,output_t):\n",
    "   #     cols.append(df.shift(-i))\n",
    "   #     names.append(i+input_t+1)\n",
    "    \n",
    "    n_df = pd.concat(cols, axis = 1)\n",
    "    n_df.columns = names\n",
    "    \n",
    "    print(\"n_df: \", n_df)\n",
    "    \n",
    "    n_df.dropna(inplace = True)\n",
    "    \n",
    "    print(\"n_df: \", n_df)\n",
    "    \n",
    "    X = n_df[[4, 3, 2, 1]]\n",
    "    Y = n_df[5]\n",
    "    ones = np.ones(shape = (X.shape[0], 1))\n",
    "    X = X / X.max()\n",
    "    X = np.append(X, ones, axis = 1)\n",
    "    Y = np.asarray(Y)\n",
    "    Y = Y / Y.max()\n",
    "\n",
    "    factor = int(.80*X.shape[0])\n",
    "\n",
    "    return split(X ,Y , factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bfcd72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "///////// prep_data_4_1_time /////////\n",
      "///////// get_cols_and_names /////////\n",
      "df.shift(4).shape:  (1082, 1)\n",
      "df.shift(3).shape:  (1082, 1)\n",
      "df.shift(2).shape:  (1082, 1)\n",
      "df.shift(1).shape:  (1082, 1)\n",
      "///////// Next loop /////////\n",
      "df.shift(0).shape:  (1082, 1)\n",
      "\n",
      "cols.shape:  (5, 1082, 1)\n",
      "names:  [4, 3, 2, 1, 5]\n",
      "len(cols) 5\n",
      "len(cols[0]) 1082\n",
      "len(cols[0][0]) 1082\n",
      "Cols:  [         0\n",
      "0      NaN\n",
      "1      NaN\n",
      "2      NaN\n",
      "3      NaN\n",
      "4     3.79\n",
      "...    ...\n",
      "1077  2.80\n",
      "1078  2.85\n",
      "1079  2.95\n",
      "1080  2.97\n",
      "1081  2.93\n",
      "\n",
      "[1082 rows x 1 columns],          0\n",
      "0      NaN\n",
      "1      NaN\n",
      "2      NaN\n",
      "3     3.79\n",
      "4     4.19\n",
      "...    ...\n",
      "1077  2.85\n",
      "1078  2.95\n",
      "1079  2.97\n",
      "1080  2.93\n",
      "1081  2.91\n",
      "\n",
      "[1082 rows x 1 columns],          0\n",
      "0      NaN\n",
      "1      NaN\n",
      "2     3.79\n",
      "3     4.19\n",
      "4     2.98\n",
      "...    ...\n",
      "1077  2.95\n",
      "1078  2.97\n",
      "1079  2.93\n",
      "1080  2.91\n",
      "1081  2.96\n",
      "\n",
      "[1082 rows x 1 columns],          0\n",
      "0      NaN\n",
      "1     3.79\n",
      "2     4.19\n",
      "3     2.98\n",
      "4     2.91\n",
      "...    ...\n",
      "1077  2.97\n",
      "1078  2.93\n",
      "1079  2.91\n",
      "1080  2.96\n",
      "1081  3.09\n",
      "\n",
      "[1082 rows x 1 columns],          0\n",
      "0     3.79\n",
      "1     4.19\n",
      "2     2.98\n",
      "3     2.91\n",
      "4     2.53\n",
      "...    ...\n",
      "1077  2.93\n",
      "1078  2.91\n",
      "1079  2.96\n",
      "1080  3.09\n",
      "1081  2.96\n",
      "\n",
      "[1082 rows x 1 columns]]\n",
      "n_df:           4     3     2     1     5\n",
      "0      NaN   NaN   NaN   NaN  3.79\n",
      "1      NaN   NaN   NaN  3.79  4.19\n",
      "2      NaN   NaN  3.79  4.19  2.98\n",
      "3      NaN  3.79  4.19  2.98  2.91\n",
      "4     3.79  4.19  2.98  2.91  2.53\n",
      "...    ...   ...   ...   ...   ...\n",
      "1077  2.80  2.85  2.95  2.97  2.93\n",
      "1078  2.85  2.95  2.97  2.93  2.91\n",
      "1079  2.95  2.97  2.93  2.91  2.96\n",
      "1080  2.97  2.93  2.91  2.96  3.09\n",
      "1081  2.93  2.91  2.96  3.09  2.96\n",
      "\n",
      "[1082 rows x 5 columns]\n",
      "n_df:           4     3     2     1     5\n",
      "4     3.79  4.19  2.98  2.91  2.53\n",
      "5     4.19  2.98  2.91  2.53  2.30\n",
      "6     2.98  2.91  2.53  2.30  1.91\n",
      "7     2.91  2.53  2.30  1.91  1.82\n",
      "8     2.53  2.30  1.91  1.82  1.86\n",
      "...    ...   ...   ...   ...   ...\n",
      "1077  2.80  2.85  2.95  2.97  2.93\n",
      "1078  2.85  2.95  2.97  2.93  2.91\n",
      "1079  2.95  2.97  2.93  2.91  2.96\n",
      "1080  2.97  2.93  2.91  2.96  3.09\n",
      "1081  2.93  2.91  2.96  3.09  2.96\n",
      "\n",
      "[1073 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.2615597 , 0.28916494, 0.20565908, 0.20082816, 1.        ],\n",
       "        [0.28916494, 0.20565908, 0.20082816, 0.17460317, 1.        ],\n",
       "        [0.20565908, 0.20082816, 0.17460317, 0.15873016, 1.        ],\n",
       "        ...,\n",
       "        [0.24430642, 0.25258799, 0.25465839, 0.25327812, 1.        ],\n",
       "        [0.25258799, 0.25465839, 0.25327812, 0.23809524, 1.        ],\n",
       "        [0.25465839, 0.25327812, 0.23809524, 0.22912353, 1.        ]]),\n",
       " array([0.17460317, 0.15873016, 0.13181504, 0.12560386, 0.12836439,\n",
       "        0.1352657 , 0.13181504, 0.12698413, 0.12974465, 0.13664596,\n",
       "        0.14078675, 0.14768806, 0.14837819, 0.15804003, 0.15320911,\n",
       "        0.15320911, 0.1573499 , 0.14975845, 0.14906832, 0.15320911,\n",
       "        0.15665977, 0.14837819, 0.14837819, 0.15458937, 0.15182885,\n",
       "        0.15320911, 0.16356108, 0.17460317, 0.17529331, 0.17805383,\n",
       "        0.19116632, 0.19047619, 0.19668737, 0.21256039, 0.20703934,\n",
       "        0.19530711, 0.1994479 , 0.22153209, 0.23602484, 0.21808144,\n",
       "        0.22429262, 0.1994479 , 0.17598344, 0.17322291, 0.16149068,\n",
       "        0.16149068, 0.15251898, 0.15596963, 0.14561767, 0.14147688,\n",
       "        0.14561767, 0.14354727, 0.1573499 , 0.15251898, 0.15182885,\n",
       "        0.15320911, 0.15044859, 0.15320911, 0.15389924, 0.15873016,\n",
       "        0.1663216 , 0.1773637 , 0.16977226, 0.16563147, 0.15389924,\n",
       "        0.14768806, 0.15182885, 0.14699793, 0.14423741, 0.14492754,\n",
       "        0.13802622, 0.14561767, 0.16425121, 0.16494134, 0.16287095,\n",
       "        0.15251898, 0.14147688, 0.13457557, 0.12836439, 0.12767426,\n",
       "        0.13319531, 0.12491373, 0.11870255, 0.126294  , 0.14285714,\n",
       "        0.15458937, 0.15182885, 0.13733609, 0.11939268, 0.13112491,\n",
       "        0.13043478, 0.14561767, 0.15665977, 0.1463078 , 0.14354727,\n",
       "        0.09247757, 0.11042098, 0.13319531, 0.13457557, 0.12698413,\n",
       "        0.13802622, 0.12491373, 0.12491373, 0.12146308, 0.12353347,\n",
       "        0.12491373, 0.12353347, 0.11732229, 0.11663216, 0.12905452,\n",
       "        0.12077295, 0.12353347, 0.13250518, 0.14078675, 0.1463078 ,\n",
       "        0.15044859, 0.15873016, 0.15873016, 0.15458937, 0.15665977,\n",
       "        0.15389924, 0.16149068, 0.16356108, 0.15665977, 0.1552795 ,\n",
       "        0.15804003, 0.15251898, 0.14768806, 0.15804003, 0.17805383,\n",
       "        0.18150449, 0.18978606, 0.19323671, 0.20565908, 0.18564527,\n",
       "        0.1863354 , 0.17805383, 0.16701173, 0.16977226, 0.16908213,\n",
       "        0.18426501, 0.20151829, 0.20289855, 0.18978606, 0.16494134,\n",
       "        0.15458937, 0.13802622, 0.15182885, 0.15251898, 0.17184265,\n",
       "        0.17460317, 0.16080055, 0.15044859, 0.1552795 , 0.16977226,\n",
       "        0.18702553, 0.19392685, 0.18426501, 0.18219462, 0.17391304,\n",
       "        0.1863354 , 0.18909593, 0.19392685, 0.19116632, 0.19875776,\n",
       "        0.20151829, 0.20910973, 0.21463078, 0.21463078, 0.2173913 ,\n",
       "        0.22498275, 0.24568668, 0.27881297, 0.30089717, 0.28916494,\n",
       "        0.29675638, 0.29537612, 0.30296756, 0.28640442, 0.28847481,\n",
       "        0.27467219, 0.25672878, 0.27674258, 0.30641822, 0.29882678,\n",
       "        0.31953071, 0.3216011 , 0.3326432 , 0.34851622, 0.35679779,\n",
       "        0.35955832, 0.35886818, 0.36300897, 0.35679779, 0.32298137,\n",
       "        0.30986888, 0.34230504, 0.39889579, 0.43409248, 0.42650104,\n",
       "        0.563147  , 0.57763975, 0.68046929, 0.68184955, 0.67563837,\n",
       "        0.66045549, 0.5300207 , 0.48792271, 0.42995169, 0.41269841,\n",
       "        0.38854382, 0.35403727, 0.3547274 , 0.36162871, 0.35334714,\n",
       "        0.35886818, 0.37198068, 0.3636991 , 0.37543133, 0.35955832,\n",
       "        0.34437543, 0.31469979, 0.29123533, 0.29744651, 0.27950311,\n",
       "        0.25810904, 0.2615597 , 0.27398206, 0.26293996, 0.22774327,\n",
       "        0.20703934, 0.22015183, 0.21187026, 0.21256039, 0.22153209,\n",
       "        0.21325052, 0.2194617 , 0.20979986, 0.16908213, 0.16149068,\n",
       "        0.16563147, 0.14837819, 0.13112491, 0.1352657 , 0.15251898,\n",
       "        0.16701173, 0.1973775 , 0.21118012, 0.18771567, 0.14906832,\n",
       "        0.15044859, 0.13802622, 0.13664596, 0.16908213, 0.16563147,\n",
       "        0.16563147, 0.17460317, 0.16425121, 0.16149068, 0.15044859,\n",
       "        0.15044859, 0.15320911, 0.1573499 , 0.16425121, 0.17046239,\n",
       "        0.18426501, 0.20151829, 0.2284334 , 0.23464458, 0.24430642,\n",
       "        0.22153209, 0.23464458, 0.24154589, 0.25189786, 0.25189786,\n",
       "        0.24637681, 0.23119393, 0.22360248, 0.22360248, 0.21463078,\n",
       "        0.22498275, 0.23671498, 0.2194617 , 0.20427881, 0.20013803,\n",
       "        0.20496894, 0.20772947, 0.19185645, 0.20634921, 0.22636301,\n",
       "        0.22981366, 0.22153209, 0.22774327, 0.25603865, 0.26224983,\n",
       "        0.28778468, 0.26570048, 0.28571429, 0.29123533, 0.29192547,\n",
       "        0.26777088, 0.26639061, 0.29330573, 0.29330573, 0.29744651,\n",
       "        0.32022084, 0.35334714, 0.34230504, 0.3347136 , 0.34713596,\n",
       "        0.37060041, 0.40786749, 0.39199448, 0.42167012, 0.41959972,\n",
       "        0.436853  , 0.83022774, 0.53830228, 0.40993789, 0.35748792,\n",
       "        0.34437543, 0.33885438, 0.35541753, 0.37957212, 0.38026225,\n",
       "        0.36231884, 0.38440304, 0.41752933, 0.41545894, 0.40234645,\n",
       "        0.43202208, 0.40993789, 0.38440304, 0.38716356, 0.3547274 ,\n",
       "        0.36991028, 0.34920635, 0.33885438, 0.32298137, 0.33333333,\n",
       "        0.34851622, 0.34989648, 0.34851622, 0.3236715 , 0.32850242,\n",
       "        0.31469979, 0.30986888, 0.30986888, 0.32574189, 0.33402346,\n",
       "        0.32436163, 0.30227743, 0.30089717, 0.31677019, 0.30089717,\n",
       "        0.32022084, 0.38509317, 0.44927536, 0.46445825, 0.40027605,\n",
       "        0.39544513, 0.4589372 , 0.41062802, 0.41890959, 0.4057971 ,\n",
       "        0.38440304, 0.37612146, 0.36645963, 0.3547274 , 0.3636991 ,\n",
       "        0.37198068, 0.38509317, 0.36645963, 0.38371291, 0.39889579,\n",
       "        0.39751553, 0.38164251, 0.39751553, 0.42097999, 0.436853  ,\n",
       "        0.436853  , 0.45341615, 0.44099379, 0.41959972, 0.44099379,\n",
       "        0.43754313, 0.41407867, 0.42305038, 0.4057971 , 0.40441684,\n",
       "        0.40786749, 0.39061422, 0.38302277, 0.36853002, 0.36093858,\n",
       "        0.33402346, 0.31469979, 0.34782609, 0.37543133, 0.39544513,\n",
       "        0.40855763, 0.38716356, 0.46238785, 0.50931677, 0.47757074,\n",
       "        0.42236025, 0.40096618, 0.35610766, 0.45617667, 0.41959972,\n",
       "        0.48516218, 0.48309179, 0.43202208, 0.39613527, 0.42167012,\n",
       "        0.44168392, 0.44168392, 0.43133195, 0.41959972, 0.41407867,\n",
       "        0.42305038, 0.4568668 , 0.47066943, 0.48930297, 0.49344375,\n",
       "        0.49827467, 0.51690821, 0.49068323, 0.48309179, 0.47964113,\n",
       "        0.45410628, 0.45479641, 0.44306418, 0.436853  , 0.4478951 ,\n",
       "        0.49068323, 0.50793651, 0.51897861, 0.48930297, 0.52726018,\n",
       "        0.53761215, 0.52795031, 0.52173913, 0.58385093, 0.62594893,\n",
       "        0.65769496, 0.67701863, 0.8005521 , 0.76880607, 0.75500345,\n",
       "        0.92891649, 0.7494824 , 0.6363009 , 0.70807453, 0.75086266,\n",
       "        0.82056591, 0.98067633, 1.        , 0.90131125, 0.68530021,\n",
       "        0.65010352, 0.59558316, 0.59834369, 0.56728778, 0.57694962,\n",
       "        0.53830228, 0.50034507, 0.5100069 , 0.46307798, 0.44444444,\n",
       "        0.48723257, 0.4899931 , 0.49068323, 0.48171153, 0.4699793 ,\n",
       "        0.52657005, 0.49482402, 0.45617667, 0.45203589, 0.40855763,\n",
       "        0.41131815, 0.42512077, 0.41821946, 0.43478261, 0.44858523,\n",
       "        0.41200828, 0.37198068, 0.39613527, 0.4168392 , 0.47066943,\n",
       "        0.55762595, 0.51276743, 0.47204969, 0.48930297, 0.4168392 ,\n",
       "        0.38164251, 0.35541753, 0.33126294, 0.28778468, 0.29813665,\n",
       "        0.34989648, 0.42926156, 0.5100069 , 0.49068323, 0.48585231,\n",
       "        0.5100069 , 0.52380952, 0.54727398, 0.51828847, 0.48378192,\n",
       "        0.43202208, 0.38647343, 0.37957212, 0.42305038, 0.44996549,\n",
       "        0.50310559, 0.5320911 , 0.57349896, 0.58178054, 0.51483782,\n",
       "        0.50655625, 0.50517598, 0.47342995, 0.47688061, 0.50517598,\n",
       "        0.521049  , 0.54037267, 0.51828847, 0.51621808, 0.5300207 ,\n",
       "        0.52173913, 0.53416149, 0.52173913, 0.52795031, 0.53554175,\n",
       "        0.51828847, 0.50793651, 0.46307798, 0.43271222, 0.44168392,\n",
       "        0.43961353, 0.39751553, 0.43478261, 0.43823326, 0.48861284,\n",
       "        0.40924776, 0.38026225, 0.39061422, 0.41614907, 0.42857143,\n",
       "        0.436853  , 0.4589372 , 0.46100759, 0.4899931 , 0.44099379,\n",
       "        0.47826087, 0.47964113, 0.49620428, 0.47481021, 0.51345756,\n",
       "        0.49137336, 0.49482402, 0.49137336, 0.48102139, 0.52242926,\n",
       "        0.5410628 , 0.57487923, 0.5431332 , 0.55348516, 0.5431332 ,\n",
       "        0.58454106, 0.61352657, 0.63216011, 0.65079365, 0.67218772,\n",
       "        0.62732919, 0.63768116, 0.66804693, 0.68184955, 0.69910283,\n",
       "        0.72739821, 0.74189096, 0.76673568, 0.78398896, 0.7805383 ,\n",
       "        0.80538302, 0.84817115, 0.86887509, 0.88888889, 0.88957902,\n",
       "        0.91097308, 0.84886128, 0.77984817, 0.68530021, 0.63146998,\n",
       "        0.60110421, 0.55900621, 0.54520359, 0.563147  , 0.52035887,\n",
       "        0.53071084, 0.54589372, 0.53416149, 0.50379572, 0.46100759,\n",
       "        0.46100759, 0.46583851, 0.44444444, 0.46652864, 0.46100759,\n",
       "        0.46031746, 0.45617667, 0.44444444, 0.39199448, 0.394755  ,\n",
       "        0.37336094, 0.38923395, 0.4057971 , 0.37405107, 0.33126294,\n",
       "        0.32712215, 0.33195307, 0.32574189, 0.29951691, 0.28640442,\n",
       "        0.29192547, 0.26846101, 0.26224983, 0.28019324, 0.24982747,\n",
       "        0.24913734, 0.24361629, 0.23809524, 0.22705314, 0.26086957,\n",
       "        0.2926156 , 0.2615597 , 0.24706694, 0.2594893 , 0.24361629,\n",
       "        0.27881297, 0.26708075, 0.25396825, 0.22567288, 0.22705314,\n",
       "        0.24154589, 0.23533471, 0.24706694, 0.23395445, 0.20772947,\n",
       "        0.18771567, 0.15044859, 0.18564527, 0.22153209, 0.23878537,\n",
       "        0.21118012, 0.2484472 , 0.27122153, 0.32436163, 0.3015873 ,\n",
       "        0.29537612, 0.23326432, 0.22774327, 0.24292616, 0.31055901,\n",
       "        0.35058661, 0.38716356, 0.39061422, 0.4057971 , 0.45272602,\n",
       "        0.39199448, 0.38371291, 0.37750173, 0.37750173, 0.38302277,\n",
       "        0.37336094, 0.33609386, 0.32712215, 0.30710835, 0.29192547,\n",
       "        0.27674258, 0.26363009, 0.27605245, 0.28019324, 0.27536232,\n",
       "        0.28709455, 0.27191166, 0.28916494, 0.29399586, 0.28847481,\n",
       "        0.30641822, 0.32643202, 0.35196687, 0.34023464, 0.3216011 ,\n",
       "        0.32091097, 0.30917874, 0.32022084, 0.32781228, 0.33126294,\n",
       "        0.30503796, 0.29675638, 0.27329193, 0.2594893 , 0.26224983,\n",
       "        0.27674258, 0.27674258, 0.2615597 , 0.24292616, 0.24085576,\n",
       "        0.23257419, 0.2284334 , 0.23395445, 0.25534852, 0.25741891,\n",
       "        0.26915114, 0.28985507, 0.30779848, 0.29399586, 0.28226363,\n",
       "        0.28709455, 0.31193927, 0.30779848, 0.31538992, 0.30710835,\n",
       "        0.31124914, 0.28778468, 0.26915114, 0.26501035, 0.26363009,\n",
       "        0.26224983, 0.26777088, 0.28433402, 0.29675638, 0.28640442,\n",
       "        0.28433402, 0.29468599, 0.30296756, 0.31055901, 0.28847481,\n",
       "        0.28640442, 0.30020704, 0.32091097, 0.33333333, 0.31469979,\n",
       "        0.29882678, 0.29813665, 0.29675638, 0.30434783, 0.31608006,\n",
       "        0.30365769, 0.2905452 , 0.2815735 , 0.27674258, 0.27674258,\n",
       "        0.27674258, 0.27329193, 0.27260179, 0.26017943, 0.26293996,\n",
       "        0.24361629, 0.24016563, 0.24982747, 0.24982747, 0.2394755 ,\n",
       "        0.23602484, 0.21394065, 0.20151829, 0.23257419, 0.23395445,\n",
       "        0.21256039, 0.20979986, 0.20979986, 0.20151829, 0.19392685,\n",
       "        0.16563147, 0.1773637 , 0.16908213, 0.17322291, 0.17391304,\n",
       "        0.18150449, 0.16908213, 0.15596963, 0.14561767, 0.14906832,\n",
       "        0.14216701, 0.13595583, 0.13319531, 0.12836439, 0.13802622,\n",
       "        0.15596963, 0.16080055, 0.17184265, 0.17874396, 0.16356108,\n",
       "        0.16080055, 0.15458937, 0.17391304, 0.19047619, 0.19599724,\n",
       "        0.19461698, 0.20151829, 0.21601104, 0.21532091, 0.20151829,\n",
       "        0.19116632, 0.19254658, 0.18771567, 0.19461698, 0.19875776,\n",
       "        0.19047619, 0.20220842, 0.22222222, 0.22498275, 0.2284334 ,\n",
       "        0.23533471, 0.23740511, 0.23464458, 0.24430642, 0.24913734,\n",
       "        0.25258799, 0.23533471, 0.22774327, 0.22774327, 0.2305038 ,\n",
       "        0.22636301, 0.2194617 , 0.23740511, 0.24430642, 0.22498275,\n",
       "        0.22981366, 0.22498275, 0.22636301, 0.24016563, 0.24637681,\n",
       "        0.25810904, 0.27536232, 0.27950311, 0.27536232, 0.28502415,\n",
       "        0.29330573, 0.2926156 , 0.29192547, 0.26846101, 0.27260179,\n",
       "        0.28571429, 0.28433402, 0.27191166, 0.26017943, 0.26708075,\n",
       "        0.25672878, 0.24430642, 0.25258799, 0.25465839, 0.25327812,\n",
       "        0.23809524, 0.22912353, 0.23119393]),\n",
       " array([[0.25327812, 0.23809524, 0.22912353, 0.23119393, 1.        ],\n",
       "        [0.23809524, 0.22912353, 0.23119393, 0.24085576, 1.        ],\n",
       "        [0.22912353, 0.23119393, 0.24085576, 0.24499655, 1.        ],\n",
       "        ...,\n",
       "        [0.20358868, 0.20496894, 0.20220842, 0.20082816, 1.        ],\n",
       "        [0.20496894, 0.20220842, 0.20082816, 0.20427881, 1.        ],\n",
       "        [0.20220842, 0.20082816, 0.20427881, 0.21325052, 1.        ]]),\n",
       " array([0.24085576, 0.24499655, 0.25120773, 0.24913734, 0.25603865,\n",
       "        0.24499655, 0.24568668, 0.25534852, 0.2615597 , 0.25465839,\n",
       "        0.24499655, 0.2394755 , 0.24982747, 0.25396825, 0.26708075,\n",
       "        0.27191166, 0.29744651, 0.29330573, 0.30710835, 0.30089717,\n",
       "        0.29744651, 0.30296756, 0.35058661, 0.36507937, 0.43823326,\n",
       "        0.45203589, 0.41407867, 0.35196687, 0.43064182, 0.31608006,\n",
       "        0.30572809, 0.30779848, 0.30710835, 0.31953071, 0.32091097,\n",
       "        0.33126294, 0.32850242, 0.32712215, 0.30779848, 0.31193927,\n",
       "        0.31193927, 0.31815045, 0.31815045, 0.3216011 , 0.31193927,\n",
       "        0.30434783, 0.28916494, 0.28364389, 0.26501035, 0.2615597 ,\n",
       "        0.2705314 , 0.26915114, 0.26570048, 0.27674258, 0.27122153,\n",
       "        0.26984127, 0.27260179, 0.26777088, 0.27743271, 0.26708075,\n",
       "        0.26363009, 0.2505176 , 0.25189786, 0.26432022, 0.28709455,\n",
       "        0.29951691, 0.28709455, 0.25879917, 0.25258799, 0.25465839,\n",
       "        0.20289855, 0.21325052, 0.20910973, 0.21256039, 0.20358868,\n",
       "        0.20082816, 0.18771567, 0.18978606, 0.20013803, 0.21601104,\n",
       "        0.20979986, 0.19047619, 0.19323671, 0.19254658, 0.18219462,\n",
       "        0.18426501, 0.18012422, 0.17943409, 0.1773637 , 0.18978606,\n",
       "        0.19875776, 0.20634921, 0.19323671, 0.18288475, 0.19185645,\n",
       "        0.20013803, 0.19323671, 0.19254658, 0.18978606, 0.1994479 ,\n",
       "        0.19875776, 0.19806763, 0.19323671, 0.19875776, 0.18978606,\n",
       "        0.18564527, 0.18564527, 0.1863354 , 0.1863354 , 0.18012422,\n",
       "        0.17184265, 0.16425121, 0.17115252, 0.16425121, 0.1463078 ,\n",
       "        0.13940649, 0.14561767, 0.14423741, 0.14837819, 0.14699793,\n",
       "        0.1352657 , 0.11870255, 0.11594203, 0.15665977, 0.16425121,\n",
       "        0.16011042, 0.15182885, 0.15320911, 0.14561767, 0.14906832,\n",
       "        0.13388544, 0.12560386, 0.10835059, 0.1131815 , 0.12353347,\n",
       "        0.12353347, 0.12836439, 0.1352657 , 0.13112491, 0.13181504,\n",
       "        0.13250518, 0.1352657 , 0.13802622, 0.13043478, 0.12698413,\n",
       "        0.14837819, 0.16287095, 0.17874396, 0.18978606, 0.1994479 ,\n",
       "        0.19461698, 0.19392685, 0.19254658, 0.19668737, 0.19875776,\n",
       "        0.19116632, 0.1884058 , 0.19461698, 0.20289855, 0.19806763,\n",
       "        0.2084196 , 0.21463078, 0.20772947, 0.20082816, 0.21877157,\n",
       "        0.21670117, 0.18771567, 0.17253278, 0.15320911, 0.16839199,\n",
       "        0.19047619, 0.22153209, 0.25465839, 0.24637681, 0.24223602,\n",
       "        0.25465839, 0.2394755 , 0.22567288, 0.22705314, 0.22567288,\n",
       "        0.21532091, 0.20979986, 0.19806763, 0.18081435, 0.17391304,\n",
       "        0.18978606, 0.20427881, 0.20634921, 0.20910973, 0.2194617 ,\n",
       "        0.21048999, 0.21394065, 0.21325052, 0.21670117, 0.21532091,\n",
       "        0.22153209, 0.2194617 , 0.20910973, 0.20565908, 0.20910973,\n",
       "        0.1994479 , 0.20772947, 0.20358868, 0.20427881, 0.21325052,\n",
       "        0.20427881, 0.19323671, 0.19668737, 0.20358868, 0.20496894,\n",
       "        0.20220842, 0.20082816, 0.20427881, 0.21325052, 0.20427881]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prep_data_4_1_time()\n",
    "#pd.DataFrame.dropna?\n",
    "#np.asarray?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13bb83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_data_8_2_time():\n",
    "    file = pd.read_csv('timesereis_8_2.csv')\n",
    "    \n",
    "    X = file[['0','1','2','3','4','5','6','7']]\n",
    "    X = X / X.max()\n",
    "    Y = file[['8','9']]\n",
    "    Y = Y / Y.max()\n",
    "\n",
    "    ones = np.ones(shape=(X.shape[0],1))\n",
    "    X = np.append(X, ones, axis=1)\n",
    "    Y = np.asarray(Y)\n",
    "    factor = int(.80*X.shape[0])\n",
    "\n",
    "    return split(X,Y,factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b77878b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf=MLPRegressor(\n",
    "#    hidden_layer_sizes = cur_hidden_layer_sizes, \n",
    "#    activation = cur_act_fun,\n",
    "#    solver = cur_solver,\n",
    "#    alpha = cur_alpha, \n",
    "#    batch_size = cur_batch_size, \n",
    "#    learning_rate = cur_learning_rate,\n",
    "#    learning_rate_init = cur_learning_rate_init, \n",
    "#    max_iter = cur_max_iter, \n",
    "#    shuffle = cur_shuffle,\n",
    "#    random_state = cur_random_state, \n",
    "#    tol = cur_tol, \n",
    "#    early_stopping = cur_early_stopping,\n",
    "#    verbose=True,\n",
    "#    validation_fraction=0.2\n",
    "#    )\n",
    "\n",
    "clf=MLPRegressor(\n",
    "    hidden_layer_sizes=(900,7), # 2 скрытых слоя, первый с 900 нейронами, второй с 7 ?\n",
    "    activation='relu', # функция активации скрытых слоев\n",
    "    solver='adam', # метод оптимизации\n",
    "    alpha=0.001, #\n",
    "    batch_size=13, # количество наборов данных, подаваемых в сеть для обучения (1,1,1 -> результат 4), (1,1,2 -> результат 2). \n",
    "    # тогда batch_size = 2\n",
    "    learning_rate='constant', # изменять ли шаг сходимости по мере обучения сети\n",
    "    learning_rate_init=0.001, # начальный шаг  сходимости\n",
    "    max_iter=1000, # макс кол-во итераций на эпоху?\n",
    "    shuffle=True, # перемешивать ли  данный в каждой итерации\n",
    "    random_state=5, # случайная генерация чисел происходит только 1 раз, все последующие разы она не меняется \n",
    "    # для весов и мб биасов\n",
    "    tol=0.001, # минимальное изменение loss для какое-то кол-во итераций (10 по умолчанию), при котором стоит \n",
    "    # продолжать обучение\n",
    "    early_stopping=True, # остановка обучения, если сеть перестала обучаться\n",
    "    verbose=True, # выводить ли в консоль прогресс обучения\n",
    "    validation_fraction = 0.2 # процент данных, отсекаемых для валидационной выборки\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480b83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLPRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec4099b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.00612746\n",
      "Validation score: 0.746442\n",
      "Iteration 2, loss = 0.00330638\n",
      "Validation score: 0.787206\n",
      "Iteration 3, loss = 0.00290018\n",
      "Validation score: 0.779019\n",
      "Iteration 4, loss = 0.00267013\n",
      "Validation score: 0.819200\n",
      "Iteration 5, loss = 0.00229647\n",
      "Validation score: 0.844220\n",
      "Iteration 6, loss = 0.00204322\n",
      "Validation score: 0.858914\n",
      "Iteration 7, loss = 0.00191468\n",
      "Validation score: 0.887999\n",
      "Iteration 8, loss = 0.00177198\n",
      "Validation score: 0.883503\n",
      "Iteration 9, loss = 0.00159402\n",
      "Validation score: 0.906087\n",
      "Iteration 10, loss = 0.00145295\n",
      "Validation score: 0.911209\n",
      "Iteration 11, loss = 0.00142983\n",
      "Validation score: 0.917060\n",
      "Iteration 12, loss = 0.00132467\n",
      "Validation score: 0.913181\n",
      "Iteration 13, loss = 0.00125178\n",
      "Validation score: 0.894917\n",
      "Iteration 14, loss = 0.00117371\n",
      "Validation score: 0.925423\n",
      "Iteration 15, loss = 0.00117044\n",
      "Validation score: 0.932130\n",
      "Iteration 16, loss = 0.00113775\n",
      "Validation score: 0.921798\n",
      "Iteration 17, loss = 0.00106412\n",
      "Validation score: 0.923834\n",
      "Iteration 18, loss = 0.00113923\n",
      "Validation score: 0.938248\n",
      "Iteration 19, loss = 0.00106557\n",
      "Validation score: 0.934124\n",
      "Iteration 20, loss = 0.00113172\n",
      "Validation score: 0.937234\n",
      "Iteration 21, loss = 0.00099493\n",
      "Validation score: 0.941435\n",
      "Iteration 22, loss = 0.00097524\n",
      "Validation score: 0.942817\n",
      "Iteration 23, loss = 0.00088102\n",
      "Validation score: 0.937176\n",
      "Iteration 24, loss = 0.00087502\n",
      "Validation score: 0.940920\n",
      "Iteration 25, loss = 0.00084173\n",
      "Validation score: 0.944978\n",
      "Iteration 26, loss = 0.00083856\n",
      "Validation score: 0.946293\n",
      "Iteration 27, loss = 0.00084664\n",
      "Validation score: 0.944781\n",
      "Iteration 28, loss = 0.00085903\n",
      "Validation score: 0.945413\n",
      "Iteration 29, loss = 0.00079952\n",
      "Validation score: 0.945537\n",
      "Iteration 30, loss = 0.00086403\n",
      "Validation score: 0.937240\n",
      "Iteration 31, loss = 0.00076760\n",
      "Validation score: 0.941801\n",
      "Iteration 32, loss = 0.00078390\n",
      "Validation score: 0.946480\n",
      "Iteration 33, loss = 0.00078899\n",
      "Validation score: 0.935747\n",
      "Iteration 34, loss = 0.00086377\n",
      "Validation score: 0.939723\n",
      "Iteration 35, loss = 0.00081099\n",
      "Validation score: 0.934605\n",
      "Iteration 36, loss = 0.00087543\n",
      "Validation score: 0.938488\n",
      "Iteration 37, loss = 0.00074597\n",
      "Validation score: 0.944934\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "score of first model 0.8888354390397906\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,X_test,Y_test=prep_data_4_1_time()\n",
    "#train the model\n",
    "clf.fit(X_train,Y_train)\n",
    "#score of the model\n",
    "print(\"score of first model\",clf.score(X_test,Y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b75e81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPRegressor.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da6b7574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of first model 0.0004252033334315175\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "pred = clf.predict(X_test)\n",
    "print(\"error of first model\",mean_squared_error(Y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb796595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.03578530\n",
      "Validation score: 0.826457\n",
      "Iteration 2, loss = 0.00279544\n",
      "Validation score: 0.839511\n",
      "Iteration 3, loss = 0.00258918\n",
      "Validation score: 0.859164\n",
      "Iteration 4, loss = 0.00247041\n",
      "Validation score: 0.870674\n",
      "Iteration 5, loss = 0.00238084\n",
      "Validation score: 0.869023\n",
      "Iteration 6, loss = 0.00226409\n",
      "Validation score: 0.885576\n",
      "Iteration 7, loss = 0.00222876\n",
      "Validation score: 0.881047\n",
      "Iteration 8, loss = 0.00211423\n",
      "Validation score: 0.875196\n",
      "Iteration 9, loss = 0.00203630\n",
      "Validation score: 0.895481\n",
      "Iteration 10, loss = 0.00192974\n",
      "Validation score: 0.906589\n",
      "Iteration 11, loss = 0.00188361\n",
      "Validation score: 0.906358\n",
      "Iteration 12, loss = 0.00186655\n",
      "Validation score: 0.906883\n",
      "Iteration 13, loss = 0.00182464\n",
      "Validation score: 0.906790\n",
      "Iteration 14, loss = 0.00178365\n",
      "Validation score: 0.910459\n",
      "Iteration 15, loss = 0.00181950\n",
      "Validation score: 0.909066\n",
      "Iteration 16, loss = 0.00186617\n",
      "Validation score: 0.904470\n",
      "Iteration 17, loss = 0.00172423\n",
      "Validation score: 0.898630\n",
      "Iteration 18, loss = 0.00177781\n",
      "Validation score: 0.910207\n",
      "Iteration 19, loss = 0.00170356\n",
      "Validation score: 0.918019\n",
      "Iteration 20, loss = 0.00169926\n",
      "Validation score: 0.901966\n",
      "Iteration 21, loss = 0.00168907\n",
      "Validation score: 0.905937\n",
      "Iteration 22, loss = 0.00173662\n",
      "Validation score: 0.906447\n",
      "Iteration 23, loss = 0.00172316\n",
      "Validation score: 0.902695\n",
      "Iteration 24, loss = 0.00167013\n",
      "Validation score: 0.897033\n",
      "Iteration 25, loss = 0.00161875\n",
      "Validation score: 0.903454\n",
      "Iteration 26, loss = 0.00157497\n",
      "Validation score: 0.912230\n",
      "Iteration 27, loss = 0.00166809\n",
      "Validation score: 0.910657\n",
      "Iteration 28, loss = 0.00173020\n",
      "Validation score: 0.893338\n",
      "Iteration 29, loss = 0.00157257\n",
      "Validation score: 0.900329\n",
      "Iteration 30, loss = 0.00162852\n",
      "Validation score: 0.905976\n",
      "Validation score did not improve more than tol=0.001000 for 10 consecutive epochs. Stopping.\n",
      "score of second model 0.8877736751383991\n"
     ]
    }
   ],
   "source": [
    "X_train,Y_train,X_test,Y_test=prep_data_8_2_time()\n",
    "#train the model\n",
    "clf.fit(X_train,Y_train)\n",
    "#score of the model\n",
    "print(\"score of second model\",clf.score(X_test,Y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02edd8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error of second model 0.0004311362990886726\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "pred = clf.predict(X_test)\n",
    "print(\"error of second model\",mean_squared_error(Y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21412dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
