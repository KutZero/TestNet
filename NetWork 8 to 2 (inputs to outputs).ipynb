{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023e5ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# детерминация случайных величин, отвечающих за выбор первоначальных весов и биасов\n",
    "tf.compat.v1.set_random_seed(290)\n",
    "tf.random.set_seed(290)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eeb34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# всякие константы для последующей работы\n",
    "\n",
    "#///////////////////////////////// для взятия данных из файлов\n",
    "\n",
    "path_to_data = 'timesereis_8_2.csv' # путь к файлу, из которого берутся данные для обучения\n",
    "\n",
    "#///////////////////////////////// для создания слоев\n",
    "\n",
    "CRT_hidden_layer_act_fun = 'relu'   # функция активация скрытых слоев и входного\n",
    "CRT_output_layer_act_fun = 'sigmoid' # функция активация выходного слоя\n",
    "\n",
    "CRT_dict = {8: CRT_hidden_layer_act_fun, \n",
    "            400: CRT_hidden_layer_act_fun, \n",
    "            20: CRT_hidden_layer_act_fun,\n",
    "            2: CRT_output_layer_act_fun} # size(layer) plus activation func\n",
    "\n",
    "#///////////////////////////////// для компиляции \n",
    "\n",
    "CMP_learning_rate = 0.00005 # шаг сходимости back propogation\n",
    "CMP_solver = keras.optimizers.Adam(CMP_learning_rate) # оптимизатор\n",
    "CMP_loss_func = 'mean_squared_error'# функция потерь\n",
    "\n",
    "#///////////////////////////////// для колбэков\n",
    "\n",
    "    # для Early_stopping\n",
    "ES_patience = 15 # кол-во эпох без улучшений\n",
    "ES_min_delta = 0.0001 # минимальное улучшение параметра за cur_patience\n",
    "ES_monitor_parametr =  'loss' # отслеживаемый параметр \n",
    "ES_save_best_weights = True # сохранять ли веса нейронки с лучшими результатами\n",
    "    \n",
    "    # для ReduceLROnPlateau\n",
    "RLPOP_monitor_parametr = 'val_loss'  # отслеживаемый параметр \n",
    "RLPOP_factor = 0.1 # множитель для расчета нового шага сходимости (new_learning_rate = old_learning_rate*RLPOP_factor)\n",
    "RLPOP_patience = 10 # кол-во эпох без улучшений\n",
    "RLPOP_verbose = 1 # выводить ли прогресс изменения шага сходимости в его процессее\n",
    "RLPOP_mode = 'auto' # выбирает, уменьшать шаг сходимости при росте величины или при её уменьшении\n",
    "RLPOP_min_delta = 0.0001 # порог изменения отслеживаемого значения\n",
    "RLPOP_cooldown = 0 # количество эпох до возобновления работы после изменения шага сходимости\n",
    "RLPOP_min_lr = 0 # минимальное значение шага сходимости\n",
    "\n",
    "    # для CallbackList\n",
    "CBL_add_history = True # вызывать ли колбэк History (если он не был довавлен вручную)\n",
    "CBL_add_progbar = True # вызывать ли колбэк ProgbarLogger (если он не был довавлен вручную)\n",
    "    \n",
    "#///////////////////////////////// для тренировки\n",
    "\n",
    "FIT_batch_size = 4 #13, 4 # размер bach при обучении/тестировании\n",
    "FIT_shuffle = True # перемешивать ли данные\n",
    "FIT_verbose = True # выводить ли прогресс обучения в его процессее\n",
    "FIT_epochs = 50 # количество эпох обучения\n",
    "FIT_validation_split = 0.2 # процент валидационных данных, отсекаемых из тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d2fa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разделить данные на тренировочные и тестовые\n",
    "def split(X,Y,factor):\n",
    "    X_train=X[:factor]\n",
    "    Y_train=Y[:factor]\n",
    "    X_test=X[factor:]\n",
    "    Y_test=Y[factor:]\n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "746b5cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# взять данные из файла\n",
    "def prep_data_8_2_time(path):\n",
    "    file = pd.read_csv(path)\n",
    "    \n",
    "    X = file[['0','1','2','3','4','5','6','7']]\n",
    "    X = X / X.max()\n",
    "    Y = file[['8','9']]\n",
    "    Y = Y / Y.max()\n",
    "    Y = np.asarray(Y)\n",
    "    \n",
    "    factor=int(.80 * X.shape[0])\n",
    "    \n",
    "    return split(X, Y, factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3537ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание архитектуры нейронки\n",
    "model = keras.Sequential()\n",
    "\n",
    "for x in CRT_dict.items():\n",
    "    model.add(Dense(x[0], activation = x[1]))\n",
    "    \n",
    "model.compile(loss = CMP_loss_func, optimizer = CMP_solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b46025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание и настройка колбэков\n",
    "callback_list = [] # массив колбэков до подачи в колбек \"callbacklist\"\n",
    "\n",
    "temp = keras.callbacks.EarlyStopping(\n",
    "            monitor = ES_monitor_parametr, \n",
    "            min_delta = ES_min_delta, \n",
    "            patience = ES_patience,\n",
    "            restore_best_weights = ES_save_best_weights\n",
    "            )\n",
    "callback_list.append(temp)\n",
    "\n",
    "temp = keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor = RLPOP_monitor_parametr, \n",
    "            factor = RLPOP_factor, \n",
    "            patience = RLPOP_patience, \n",
    "            verbose = RLPOP_verbose,\n",
    "            mode = RLPOP_mode, \n",
    "            min_delta = RLPOP_min_delta, \n",
    "            cooldown = RLPOP_cooldown, \n",
    "            min_lr = RLPOP_min_lr\n",
    "            )\n",
    "callback_list.append(temp)\n",
    "\n",
    "FIT_callback_list = keras.callbacks.CallbackList(\n",
    "            callbacks = callback_list, \n",
    "            add_history = CBL_add_history, \n",
    "            add_progbar = CBL_add_progbar, \n",
    "            model = model\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d39bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# подготовка данных\n",
    "\n",
    "X_train,Y_train,X_test,Y_test = prep_data_8_2_time(path_to_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b44883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "171/171 [==============================] - 0s 800us/step - loss: 0.0508 - val_loss: 0.0518\n",
      "171/171 [==============================] - 0s 436us/step - loss: 0.0463 - val_loss: 0.0433\n",
      "171/171 [==============================] - 0s 378us/step - loss: 0.0430 - val_loss: 0.0376\n",
      "171/171 [==============================] - 0s 414us/step - loss: 0.0397 - val_loss: 0.0334\n",
      "171/171 [==============================] - 0s 405us/step - loss: 0.0361 - val_loss: 0.0304\n",
      "171/171 [==============================] - 0s 345us/step - loss: 0.0320 - val_loss: 0.0266\n",
      "137/171 [=======================>......] - ETA: 0s - loss: 0.0289"
     ]
    }
   ],
   "source": [
    "# тренировка модели\n",
    "history = model.fit(\n",
    "            X_train, \n",
    "            Y_train, \n",
    "            batch_size = FIT_batch_size, \n",
    "            epochs = FIT_epochs, \n",
    "            verbose = FIT_verbose, \n",
    "            validation_split = FIT_validation_split, \n",
    "            shuffle = FIT_shuffle, \n",
    "            callbacks = FIT_callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6c1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e53de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# тест модели\n",
    "model.evaluate(X_test, Y_test, batch_size = FIT_batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
